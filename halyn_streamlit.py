import streamlit as st
from PIL import Image
import torch
import time
import numpy as np
import torchxrayvision as xrv
import torch.nn.functional as F
from transformers import AutoModelForCausalLM
from torchvision.transforms import transforms

# ì œëª© ì„¤ì •
st.title("ğŸ« Deep Chest DoctorğŸ©ºğŸ§‘ğŸ»â€âš•ï¸ğŸ‘©ğŸ»â€âš•ï¸")

# ì‚¬ì´ë“œë°”ì— íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯ ì¶”ê°€
uploaded_file = st.sidebar.file_uploader("Choose a chest X-ray image...", type=["png", "jpg", "jpeg"])

### ì—¬ê¸°ì— ëª¨ë¸ ë¡œë“œí•˜ê¸°
vision_model = xrv.models.DenseNet(weights="densenet121-res224-all")
# model_path = "alpaca-lora/24040529"  # ì–˜ê°€ ìµœê·¼ ëª¨ë¸ì¸ë° ì˜ ì•ˆë˜ëŠ”ë“¯í•˜ê¸°ë„í•˜ê³ .. ëª¨ë¥´ê² ìŠ´
model_path = "alpaca-lora/lora-alpaca/checkpoint-2000"
model = AutoModelForCausalLM.from_pretrained(model_path)

### ì—¬ê¸°ì— ì´ë¯¸ì§€ ì„ë² ë”© í•¨ìˆ˜ ì²˜ë¦¬ë¥¼ í–ˆëŠ”ë° ì´ìƒí•´ì—¬
def embed_visual_data(image,vision_model):
    image = np.array(image)
    image_tensor = torch.from_numpy(image).float()  

    if image_tensor.ndim == 3 and image_tensor.shape[0] != 3: 
        image_tensor = image_tensor.permute(2, 0, 1)
    image_tensor = (image_tensor + 1024) / 2048


    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)), 
        transforms.ToTensor()  
    ])

    # Apply the transform
    image_tensor = transform(image_tensor)

    # Unsqueeze to add the batch dimension
    image_tensor = image_tensor.unsqueeze(0)
    image_tensor = image_tensor[:,0,:,:]
    image_tensor = image_tensor.unsqueeze(0)
    feats = vision_model.features(image)
    feats = F.relu(feats, inplace=True)
    feats = F.adaptive_avg_pool2d(feats, (1, 1))
    feats = feats.reshape(-1)
    return feats

if uploaded_file is not None:
    # ì´ë¯¸ì§€ë¥¼ ì—´ê³  í™”ë©´ì— í‘œì‹œ
    image = Image.open(uploaded_file)

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    col1, col2 = st.columns(2)

    with col1:
        st.image(image, caption='Uploaded Chest X-ray.', use_column_width=True)

    with col2:
        st.write("Classifying...")
        start_time = time.time()

        # ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ì „ì²˜ë¦¬
        inputs = embed_visual_data(image, vision_model)
        print(inputs)

        with torch.no_grad():
            outputs = model(**inputs)

        
"""
ê²°ê³¼ í‘œì‹œí•˜ëŠ” ë¶€ë¶„ë„ ì§œì•¼í• ê²ƒê°™ìŠµë‹ˆë‹¹
ì´ì •ì´ê°€ ì§œë†“ì€ê²Œ CLIP/MedCLIPíŠ¹í™”ëœ ëŠë‚Œìœ¼ë¡œ promptê°€ ë‚˜ì˜¤ëŠ”ê±°ë¼ì„œ,
ìš°ë¦¬ ëª¨ë¸ì˜ ê²°ê³¼ë¡œ ì–´ë–»ê²Œ ë‹µë³€ì´ ë‚˜ì˜¬ì§€ ë‹¤ì‹œ ì§œì•¼í• ê²ƒê°™ì€ë° ë¨¸ë¦¬ê°€ ì•ˆëŒì•„ê°€ì—¬

"""


        